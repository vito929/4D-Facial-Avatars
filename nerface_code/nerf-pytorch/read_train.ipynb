{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8e39f9-79e3-4af1-b921-5713dccf8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, './nerf')\n",
    "os.environ['GPU_DEBUG']='3'\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import yaml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nerf.load_flame import load_flame_data\n",
    "\n",
    "from nerf import (CfgNode, get_embedding_function, get_ray_bundle, img2mse,\n",
    "                  load_llff_data, meshgrid_xy, models,\n",
    "                  mse2psnr, run_one_iter_of_nerf, dump_rays, GaussianSmoothing)\n",
    "#from gpu_profile import gpu_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b85ae81-20c4-405e-8b81-f15494572a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'type': 'blender', 'basedir': 'nerface_dataset/person_1', 'half_res': False, 'testskip': 1, 'no_ndc': True, 'near': 0.2, 'far': 0.8})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = 'nerface_dataset/person_1/person_1_config.yml'\n",
    "cfg = None\n",
    "with open(config, \"r\") as f:\n",
    "    cfg_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    cfg = CfgNode(cfg_dict)\n",
    "cfg.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "775954a7-6f37-42cf-a75c-acfd25692e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment': {'id': 'person_1',\n",
       "  'logdir': 'logs/',\n",
       "  'randomseed': 42,\n",
       "  'train_iters': 1000000,\n",
       "  'validate_every': 1000,\n",
       "  'save_every': 5000,\n",
       "  'print_every': 100,\n",
       "  'device': 0},\n",
       " 'dataset': {'type': 'blender',\n",
       "  'basedir': 'nerface_dataset/person_1',\n",
       "  'half_res': False,\n",
       "  'testskip': 1,\n",
       "  'no_ndc': True,\n",
       "  'near': 0.2,\n",
       "  'far': 0.8},\n",
       " 'models': {'coarse': {'type': 'ConditionalBlendshapePaperNeRFModel',\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 256,\n",
       "   'skip_connect_every': 3,\n",
       "   'include_input_xyz': True,\n",
       "   'log_sampling_xyz': True,\n",
       "   'num_encoding_fn_xyz': 10,\n",
       "   'use_viewdirs': True,\n",
       "   'include_input_dir': False,\n",
       "   'num_encoding_fn_dir': 4,\n",
       "   'log_sampling_dir': True},\n",
       "  'fine': {'type': 'ConditionalBlendshapePaperNeRFModel',\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 256,\n",
       "   'skip_connect_every': 3,\n",
       "   'num_encoding_fn_xyz': 10,\n",
       "   'include_input_xyz': True,\n",
       "   'log_sampling_xyz': True,\n",
       "   'use_viewdirs': True,\n",
       "   'include_input_dir': False,\n",
       "   'num_encoding_fn_dir': 4,\n",
       "   'log_sampling_dir': True}},\n",
       " 'optimizer': {'type': 'Adam', 'lr': 0.0005},\n",
       " 'scheduler': {'lr_decay': 250, 'lr_decay_factor': 0.1},\n",
       " 'nerf': {'use_viewdirs': True,\n",
       "  'encode_position_fn': 'positional_encoding',\n",
       "  'encode_direction_fn': 'positional_encoding',\n",
       "  'train': {'num_random_rays': 2048,\n",
       "   'chunksize': 2048,\n",
       "   'perturb': True,\n",
       "   'num_coarse': 64,\n",
       "   'num_fine': 64,\n",
       "   'white_background': False,\n",
       "   'radiance_field_noise_std': 0.1,\n",
       "   'lindisp': False},\n",
       "  'validation': {'chunksize': 65536,\n",
       "   'perturb': True,\n",
       "   'num_coarse': 64,\n",
       "   'num_fine': 64,\n",
       "   'white_background': False,\n",
       "   'radiance_field_noise_std': 0.0,\n",
       "   'lindisp': False}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e495b30-6c08-45f8-85e3-dcdb1f248ee3",
   "metadata": {},
   "source": [
    "### `load_flame_data`\n",
    "`def load_flame_data(basedir, half_res=False, testskip=1, debug=False,expressions=True,load_frontal_faces=False, load_bbox=True, test=False)`  \n",
    "- basedir: path\n",
    "- half_res:\n",
    "- testskip: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85edf807-3115-4bbf-b1d8-f38430861f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting data loading\n",
      "Done with data loading\n",
      "done loading data\n"
     ]
    }
   ],
   "source": [
    "images, poses, render_poses, hwf, expressions = None, None, None, None, None\n",
    "if cfg.dataset.type.lower() == \"blender\":\n",
    "    images, poses, render_poses, hwf, i_split, expressions, _, bboxs = load_flame_data(\n",
    "        cfg.dataset.basedir,\n",
    "        half_res=cfg.dataset.half_res,\n",
    "        testskip=cfg.dataset.testskip,\n",
    "    )\n",
    "    i_train, i_val, i_test = i_split\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "    if cfg.nerf.train.white_background:\n",
    "        images = images[..., :3] * images[..., -1:] + (1.0 - images[..., -1:])\n",
    "print(\"done loading data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcc5c417-4494-4b7d-8877-bc433963af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "metas = {}\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for s in splits:\n",
    "    with open(os.path.join(cfg.dataset.basedir, f\"transforms_{s}.json\"), \"r\") as fp:\n",
    "        metas[s] = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d971db8-2e30-4554-a95a-160ca3743a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nerface_dataset/person_1/transforms_test.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(cfg.dataset.basedir, f\"transforms_{s}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4702cd97-86bb-4e21-9c4f-f501b888a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.device_count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-2022.05-215]",
   "language": "python",
   "name": "conda-env-.conda-2022.05-215-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
