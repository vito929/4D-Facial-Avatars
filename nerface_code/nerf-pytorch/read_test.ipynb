{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3df23142-8f74-465a-aa4a-cc4bb58b868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "#from nerf-pytorch import\n",
    "\n",
    "\n",
    "from nerf import (\n",
    "    CfgNode,\n",
    "    get_ray_bundle,\n",
    "    load_flame_data,\n",
    "    load_llff_data,\n",
    "    models,\n",
    "    get_embedding_function,\n",
    "    run_one_iter_of_nerf,\n",
    "    meshgrid_xy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b533c92-74c9-433a-a679-3326a68cd93b",
   "metadata": {},
   "source": [
    "### read config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f143d8c3-ab2b-4dfa-83ec-0beb56db19b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blender'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = 'nerface_dataset/person_2/person_2_config.yml'\n",
    "cfg = None\n",
    "with open(config, \"r\") as f:\n",
    "    cfg_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    cfg = CfgNode(cfg_dict)\n",
    "cfg.dataset.type.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f971a1c-4ba7-4140-8ed0-080be1f185f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting data loading\n",
      "Done with data loading\n"
     ]
    }
   ],
   "source": [
    "images, poses, render_poses, hwf = None, None, None, None\n",
    "i_train, i_val, i_test = None, None, None\n",
    "if cfg.dataset.type.lower() == \"blender\":\n",
    "    # Load blender dataset\n",
    "    images, poses, render_poses, hwf, i_split, expressions, _, _ = load_flame_data(\n",
    "        cfg.dataset.basedir,\n",
    "        half_res=cfg.dataset.half_res,\n",
    "        testskip=cfg.dataset.testskip,\n",
    "        test=True\n",
    "    )\n",
    "    #i_train, i_val, i_test = i_split\n",
    "    i_test = i_split\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "elif cfg.dataset.type.lower() == \"llff\":\n",
    "    # Load LLFF dataset\n",
    "    images, poses, bds, render_poses, i_test = load_llff_data(\n",
    "        cfg.dataset.basedir, factor=cfg.dataset.downsample_factor,\n",
    "    )\n",
    "    hwf = poses[0, :3, -1]\n",
    "    H, W, focal = hwf\n",
    "    hwf = [int(H), int(W), focal]\n",
    "    render_poses = torch.from_numpy(render_poses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f5c625-030a-4aca-b94d-aa5fb614e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device on which to run.\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(device)\n",
    "encode_position_fn = get_embedding_function(\n",
    "    num_encoding_functions=cfg.models.coarse.num_encoding_fn_xyz,\n",
    "    include_input=cfg.models.coarse.include_input_xyz,\n",
    "    log_sampling=cfg.models.coarse.log_sampling_xyz,\n",
    ")\n",
    "\n",
    "encode_direction_fn = None\n",
    "if cfg.models.coarse.use_viewdirs:\n",
    "    encode_direction_fn = get_embedding_function(\n",
    "        num_encoding_functions=cfg.models.coarse.num_encoding_fn_dir,\n",
    "        include_input=cfg.models.coarse.include_input_dir,\n",
    "        log_sampling=cfg.models.coarse.log_sampling_dir,\n",
    "    )\n",
    "\n",
    "# Initialize a coarse resolution model.\n",
    "model_coarse = getattr(models, cfg.models.coarse.type)(\n",
    "    num_encoding_fn_xyz=cfg.models.coarse.num_encoding_fn_xyz,\n",
    "    num_encoding_fn_dir=cfg.models.coarse.num_encoding_fn_dir,\n",
    "    include_input_xyz=cfg.models.coarse.include_input_xyz,\n",
    "    include_input_dir=cfg.models.coarse.include_input_dir,\n",
    "    use_viewdirs=cfg.models.coarse.use_viewdirs,\n",
    "    num_layers=cfg.models.coarse.num_layers,\n",
    "    hidden_size=cfg.models.coarse.hidden_size,\n",
    "    include_expression=True\n",
    ")\n",
    "model_coarse.to(device)\n",
    "\n",
    "# If a fine-resolution model is specified, initialize it.\n",
    "model_fine = None\n",
    "if hasattr(cfg.models, \"fine\"):\n",
    "    model_fine = getattr(models, cfg.models.fine.type)(\n",
    "        num_encoding_fn_xyz=cfg.models.fine.num_encoding_fn_xyz,\n",
    "        num_encoding_fn_dir=cfg.models.fine.num_encoding_fn_dir,\n",
    "        include_input_xyz=cfg.models.fine.include_input_xyz,\n",
    "        include_input_dir=cfg.models.fine.include_input_dir,\n",
    "        use_viewdirs=cfg.models.fine.use_viewdirs,\n",
    "        num_layers=cfg.models.coarse.num_layers,\n",
    "        hidden_size=cfg.models.coarse.hidden_size,\n",
    "        include_expression=True\n",
    "    )\n",
    "    model_fine.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1c8b5-d643-4dc2-b7aa-3f9e66b4975a",
   "metadata": {},
   "source": [
    "### read ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75746aa6-ba20-4375-a349-f702ad2e49b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['iter', 'model_coarse_state_dict', 'model_fine_state_dict', 'optimizer_state_dict', 'loss', 'psnr', 'background', 'latent_codes'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = 'logs/person_3/checkpoint400000.ckpt'\n",
    "checkpoint = torch.load(ckpt)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f802ac6-a268-4836-9a3d-9b58b98867b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded background with shape  torch.Size([512, 512, 3])\n",
      "loading index map for latent codes...\n",
      "loaded latent codes from checkpoint, with shape  torch.Size([5435, 32])\n",
      "idx_map shape (5318, 2)\n"
     ]
    }
   ],
   "source": [
    "model_coarse.load_state_dict(checkpoint[\"model_coarse_state_dict\"])\n",
    "if checkpoint[\"model_fine_state_dict\"]:\n",
    "    try:\n",
    "        model_fine.load_state_dict(checkpoint[\"model_fine_state_dict\"])\n",
    "    except:\n",
    "        print(\n",
    "            \"The checkpoint has a fine-level model, but it could \"\n",
    "            \"not be loaded (possibly due to a mismatched config file.\"\n",
    "        )\n",
    "if \"height\" in checkpoint.keys():\n",
    "    hwf[0] = checkpoint[\"height\"]\n",
    "if \"width\" in checkpoint.keys():\n",
    "    hwf[1] = checkpoint[\"width\"]\n",
    "if \"focal_length\" in checkpoint.keys():\n",
    "    hwf[2] = checkpoint[\"focal_length\"]\n",
    "if \"background\" in checkpoint.keys():\n",
    "    background = checkpoint[\"background\"]\n",
    "    if background is not None:\n",
    "        print(\"loaded background with shape \", background.shape)\n",
    "        background.to(device)\n",
    "if \"latent_codes\" in checkpoint.keys():\n",
    "    latent_codes = checkpoint[\"latent_codes\"]\n",
    "    use_latent_code = False\n",
    "    if latent_codes is not None:\n",
    "        use_latent_code = True\n",
    "        latent_codes.to(device)\n",
    "        print(\"loading index map for latent codes...\")\n",
    "        idx_map = np.load(cfg.dataset.basedir + \"/index_map.npy\").astype(int)\n",
    "        print(\"loaded latent codes from checkpoint, with shape \", latent_codes.shape)\n",
    "        print(\"idx_map shape\", idx_map.shape)\n",
    "model_coarse.eval()\n",
    "if model_fine:\n",
    "    model_fine.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2f5e0fd-ea0e-418d-aaba-a3bfaa6b6962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 3141],\n",
       "       [   1,  571],\n",
       "       [   2, 1928],\n",
       "       ...,\n",
       "       [5315, 2805],\n",
       "       [5316, 3215],\n",
       "       [5317, 3059]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93b45d61-3921-49d9-b087-3d33bf9c2b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded custom background of shape torch.Size([512, 512, 3])\n"
     ]
    }
   ],
   "source": [
    "replace_background = True\n",
    "if replace_background:\n",
    "    from PIL import Image\n",
    "    #background = Image.open('./view.png')\n",
    "    background = Image.open(cfg.dataset.basedir + '/bg/00050.png')\n",
    "    #background = Image.open(\"./real_data/andrei_dvp/\" + '/bg/00050.png')\n",
    "    background.thumbnail((H,W))\n",
    "    background = torch.from_numpy(np.array(background).astype(float)).to(device)\n",
    "    background = background/255\n",
    "    print('loaded custom background of shape', background.shape)\n",
    "\n",
    "    #background = torch.ones_like(background)\n",
    "    #background.permute(2,0,1)\n",
    "\n",
    "render_poses = render_poses.float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdaa6c6-0433-4eb7-812c-b22fa20a2150",
   "metadata": {},
   "source": [
    "### Create directory to save images to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e51680a-3e90-426c-b7cb-d8f6252c9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = 'renders/person_2_rendered_frames_jupyter'\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "# if configargs.save_disparity_image:\n",
    "#     os.makedirs(os.path.join(configargs.savedir, \"disparity\"), exist_ok=True)\n",
    "# if configargs.save_error_image:\n",
    "#     os.makedirs(os.path.join(configargs.savedir, \"error\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(savedir, \"normals\"), exist_ok=True)\n",
    "# Evaluation loop\n",
    "times_per_image = []\n",
    "\n",
    "#render_poses = render_poses.float().to(device)\n",
    "render_poses = poses[i_test].float().to(device)\n",
    "#expressions = torch.arange(-6,6,0.5).float().to(device)\n",
    "render_expressions = expressions[i_test].float().to(device)\n",
    "#avg_img = torch.mean(images[i_train],axis=0)\n",
    "#avg_img = torch.ones_like(avg_img)\n",
    "\n",
    "#pose = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "#for i, pose in enumerate(tqdm(render_poses)):\n",
    "index_of_image_after_train_shuffle = 0\n",
    "# render_expressions = render_expressions[[300]] ### TODO render specific expression\n",
    "\n",
    "#######################\n",
    "no_background = False\n",
    "no_expressions = False\n",
    "no_lcode = False\n",
    "nerf = False\n",
    "frontalize = False\n",
    "interpolate_mouth = False\n",
    "\n",
    "#######################\n",
    "if nerf:\n",
    "    no_background = True\n",
    "    no_expressions = True\n",
    "    no_lcode = True\n",
    "if no_background: background=None\n",
    "if no_expressions: render_expressions = torch.zeros_like(render_expressions, device=render_expressions.device)\n",
    "if no_lcode:\n",
    "    use_latent_code = True\n",
    "    latent_codes = torch.zeros(5000,32,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88280897-e034-472d-8b20-d50cf9398a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render_expressions shape:  torch.Size([1000, 76])\n",
      "render_poses shape:  torch.Size([1000, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "#print\n",
    "\n",
    "# print(\"expression shape: \", expressions.shape)\n",
    "print(\"render_expressions shape: \", render_expressions.shape)\n",
    "# print(\"poses shape: \", poses.shape)\n",
    "print(\"render_poses shape: \", render_poses.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911cfcc-9503-4f0f-9a41-28c7e53ce966",
   "metadata": {},
   "source": [
    "# class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03be1dae-3611-42d0-a41b-eb6ea30e7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_normal_map(depthmap,focal,weights=None,clean=True, central_difference=False):\n",
    "    W,H = depthmap.shape\n",
    "    #normals = torch.zeros((H,W,3), device=depthmap.device)\n",
    "    cx = focal[2]*W\n",
    "    cy = focal[3]*H\n",
    "    fx = focal[0]\n",
    "    fy = focal[1]\n",
    "    ii, jj = meshgrid_xy(torch.arange(W, device=depthmap.device),\n",
    "                         torch.arange(H, device=depthmap.device))\n",
    "    points = torch.stack(\n",
    "        [\n",
    "            ((ii - cx) * depthmap) / fx,\n",
    "            -((jj - cy) * depthmap) / fy,\n",
    "            depthmap,\n",
    "        ],\n",
    "        dim=-1)\n",
    "    difference = 2 if central_difference else 1\n",
    "    dx = (points[difference:,:,:] - points[:-difference,:,:])\n",
    "    dy = (points[:,difference:,:] - points[:,:-difference,:])\n",
    "    normals = torch.cross(dy[:-difference,:,:],dx[:,:-difference,:],2)\n",
    "    normalize_factor = torch.sqrt(torch.sum(normals*normals,2))\n",
    "    normals[:,:,0]  /= normalize_factor\n",
    "    normals[:,:,1]  /= normalize_factor\n",
    "    normals[:,:,2]  /= normalize_factor\n",
    "    normals = normals * 0.5 +0.5\n",
    "\n",
    "    if clean and weights is not None: # Use volumetric rendering weights to clean up the normal map\n",
    "        mask = weights.repeat(3,1,1).permute(1,2,0)\n",
    "        mask = mask[:-difference,:-difference]\n",
    "        where = torch.where(mask > 0.22)\n",
    "        normals[where] = 1.0\n",
    "        normals = (1-mask)*normals + (mask)*torch.ones_like(normals)\n",
    "    normals *= 255\n",
    "    #plt.imshow(normals.cpu().numpy().astype('uint8'))\n",
    "    #plt.show()\n",
    "    return normals\n",
    "\n",
    "def save_plt_image(im1, outname):\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches((6.4,6.4))\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    #plt.set_cmap('jet')\n",
    "    ax.imshow(im1, aspect='equal')\n",
    "    plt.savefig(outname, dpi=80)\n",
    "    plt.close(fig)\n",
    "    \n",
    "def cast_to_image(tensor, dataset_type):\n",
    "    # Input tensor is (H, W, 3). Convert to (3, H, W).\n",
    "    tensor = tensor.permute(2, 0, 1)\n",
    "    tensor = tensor.clamp(0.0,1.0)\n",
    "    # Convert to PIL Image and then np.array (output shape: (H, W, 3))\n",
    "    img = np.array(torchvision.transforms.ToPILImage()(tensor.detach().cpu()))\n",
    "    return img\n",
    "    # # Map back to shape (3, H, W), as tensorboard needs channels first.\n",
    "    # return np.moveaxis(img, [-1], [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fe826-656a-4430-b792-8afb1b9c81ce",
   "metadata": {},
   "source": [
    "# test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5edb43b2-b9bd-4266-a71f-985bd51e6db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 76])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = render_expressions[760:761,:]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99483bed-f2fb-42d3-9507-1e0c21aa68a4",
   "metadata": {},
   "source": [
    "### 1 eopch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f8539bd-3fe9-43ec-b604-bfe89f600bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg time per image: 13.79657769203186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, expression in enumerate(tqdm(sample)):\n",
    "        start = time.time()\n",
    "        rgb = None, None\n",
    "        disp = None, None\n",
    "        with torch.no_grad():\n",
    "            pose = render_poses[i]\n",
    "            \n",
    "            #---?---\n",
    "            ablate = 'None'\n",
    "\n",
    "            if ablate == 'expression':\n",
    "                pose = render_poses[100]\n",
    "            elif ablate == 'latent_code':\n",
    "                pose = render_poses[100]\n",
    "                expression = render_expressions[100]\n",
    "                if idx_map[100+i,1] >= 0:\n",
    "                    #print(\"found latent code for this image\")\n",
    "                    index_of_image_after_train_shuffle = idx_map[100+i,1]\n",
    "            elif ablate == 'view_dir':\n",
    "                pose = render_poses[100]\n",
    "                expression = render_expressions[100]\n",
    "                _, ray_directions_ablation = get_ray_bundle(hwf[0], hwf[1], hwf[2], render_poses[240+i][:3, :4])\n",
    "\n",
    "            # pose = pose[:3, :4]\n",
    "\n",
    "            ## --------------Latent code: ---------------\n",
    "            if use_latent_code:\n",
    "                if idx_map[i,1] >= 0:\n",
    "                    #print(\"found latent code for this image\")\n",
    "                    index_of_image_after_train_shuffle = idx_map[i,1]\n",
    "            #index_of_image_after_train_shuffle = 10 ## TODO Fixes latent code\n",
    "            #index_of_image_after_train_shuffle = idx_map[84,1] ## TODO Fixes latent code v2 for andrei\n",
    "            index_of_image_after_train_shuffle = idx_map[10,1] ## TODO Fixes latent code - USE THIS if not ablating!\n",
    "\n",
    "                # shape [32] \n",
    "            latent_code = latent_codes[index_of_image_after_train_shuffle].to(device) if use_latent_code else None\n",
    "            ## --------------Latent code ends ---------------\n",
    "            \n",
    "            ray_origins, ray_directions = get_ray_bundle(hwf[0], hwf[1], hwf[2], pose)\n",
    "            rgb_coarse, disp_coarse, _, rgb_fine, disp_fine, _, weights = run_one_iter_of_nerf(\n",
    "                hwf[0],\n",
    "                hwf[1],\n",
    "                hwf[2],\n",
    "                model_coarse,\n",
    "                model_fine,\n",
    "                ray_origins,\n",
    "                ray_directions,\n",
    "                cfg,\n",
    "                mode=\"validation\",\n",
    "                encode_position_fn=encode_position_fn,\n",
    "                encode_direction_fn=encode_direction_fn,\n",
    "                expressions = expression,\n",
    "                background_prior = background.view(-1,3) if (background is not None) else None,\n",
    "                #background_prior = torch.ones_like(background).view(-1,3),  # White background\n",
    "                latent_code = latent_code,\n",
    "                ray_directions_ablation = None\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## ----------------calculate time \n",
    "            times_per_image.append(time.time() - start)\n",
    "            tqdm.write(f\"Avg time per image: {sum(times_per_image) / (i + 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9df85f75-e1ce-46ec-a410-b099cee3e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------generate 'normals' image------------\n",
    "rgb = rgb_fine if rgb_fine is not None else rgb_coarse\n",
    "normals = torch_normal_map(disp_fine, focal, weights, clean=True)\n",
    "#normals = normal_map_from_depth_map_backproject(disp_fine.cpu().numpy())\n",
    "save_plt_image(normals.cpu().numpy().astype('uint8'), os.path.join(savedir, 'normals', f\"{i:04d}.png\"))\n",
    "#if configargs.save_disparity_image:\n",
    "if False:\n",
    "    disp = disp_fine if disp_fine is not None else disp_coarse\n",
    "    #normals = normal_map_from_depth_map_backproject(disp.cpu().numpy())\n",
    "    normals = normal_map_from_depth_map_backproject(disp_fine.cpu().numpy())\n",
    "    save_plt_image(normals.astype('uint8'), os.path.join(configargs.savedir,'normals', f\"{i:04d}.png\"))\n",
    "##------------------generate 'normals' image ends ------------\n",
    "##------------------save image------------------------------------\n",
    "if savedir:\n",
    "    savefile = os.path.join(savedir, f\"{i:04d}.png\")\n",
    "    imageio.imwrite(\n",
    "        savefile, cast_to_image(rgb[..., :3], cfg.dataset.type.lower())\n",
    "    )\n",
    "    # if configargs.save_disparity_image:\n",
    "    #     savefile = os.path.join(configargs.savedir, \"disparity\", f\"{i:04d}.png\")\n",
    "    #     imageio.imwrite(savefile, cast_to_disparity_image(disp_fine))\n",
    "    # if configargs.save_error_image:\n",
    "    #     savefile = os.path.join(configargs.savedir, \"error\", f\"{i:04d}.png\")\n",
    "    #     GT = images[i_test][i]\n",
    "    #     fig = error_image(GT, rgb.cpu().numpy())\n",
    "    #     #imageio.imwrite(savefile, cast_to_disparity_image(disp))\n",
    "    #     plt.savefig(savefile,pad_inches=0,bbox_inches='tight',dpi=54)\n",
    "##------------------save image ends------------------------------------        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d294db21-d7e6-4ebc-a71d-3ccf02f34809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9960, -0.0099, -0.0890, -0.0479],\n",
      "        [ 0.0194,  0.9941,  0.1071,  0.1003],\n",
      "        [ 0.0874, -0.1084,  0.9903,  0.5183],\n",
      "        [-0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "tensor([[ 0.9960, -0.0099, -0.0890, -0.0479],\n",
      "        [ 0.0194,  0.9941,  0.1071,  0.1003],\n",
      "        [ 0.0874, -0.1084,  0.9903,  0.5183]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pose = render_poses[i]\n",
    "print(pose)\n",
    "pose = pose[:3, :4]\n",
    "print(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc007603-9f54-41f4-b43c-0de5c795f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro = ray_origins.view((-1, 3))\n",
    "rd = ray_directions.view((-1, 3))\n",
    "near = cfg.dataset.near * torch.ones_like(rd[..., :1])\n",
    "far = cfg.dataset.far * torch.ones_like(rd[..., :1])\n",
    "rays = torch.cat((ro, rd, near, far), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69ed1bd5-a5c0-4047-88e7-4fdc2b6fc708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd1f6efc-df88-4ed7-a654-65e301b020e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 76])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_expressions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "432b7f58-3962-43cc-8dce-3ac5990653e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([512, 512, 3]), torch.Size([512, 512]), torch.Size([512, 512])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    restore_shapes = [\n",
    "        ray_directions.shape,\n",
    "        ray_directions.shape[:-1],\n",
    "        ray_directions.shape[:-1],\n",
    "    ]\n",
    "    restore_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b556d2-af52-4ae8-b669-25222795db92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbdd3158-af2d-4c81-820a-35ec8d5a10c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82648cbd-40d8-4d81-bb3a-c995cc5dc95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0008, -0.0012,  0.0016,  0.0048,  0.0019,  0.0028, -0.0037, -0.0016,\n",
      "        -0.0117, -0.0039,  0.0035,  0.0012,  0.0096, -0.0092, -0.0006, -0.0067,\n",
      "        -0.0002, -0.0017,  0.0042,  0.0006,  0.0018, -0.0069,  0.0007, -0.0007,\n",
      "         0.0014,  0.0061, -0.0032, -0.0011, -0.0011, -0.0017, -0.0043,  0.0026],\n",
      "       device='cuda:0')\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(latent_code)\n",
    "print(latent_code.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca9cd56b-f770-4d94-bd87-3c44523ea6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.0813e-05,  7.0033e-02,  2.5274e-01, -9.2142e-02, -1.3292e-01,\n",
      "         2.1672e-02,  7.1768e-02, -7.6453e-02, -2.0912e-01,  1.3151e-03,\n",
      "         1.3103e-01,  2.9060e-01,  2.8297e-02, -5.2820e-02,  1.9694e-01,\n",
      "         7.0626e-02, -1.1306e-02,  5.7903e-02, -2.1446e-01,  9.0488e-02,\n",
      "         2.9039e-02,  2.9313e-02,  3.5732e-02, -4.8023e-02,  3.5983e-02,\n",
      "         1.9504e-01,  3.3352e-02,  1.2454e-01,  4.5865e-02,  9.8833e-02,\n",
      "         2.9979e-02,  6.5397e-02,  4.0572e-01, -4.6820e-02, -3.7146e-01,\n",
      "        -2.0494e-02,  8.1898e-02,  4.1465e-02,  3.5226e-02,  3.0267e-02,\n",
      "         1.9230e-01, -1.7592e-03,  1.8759e-02, -1.9538e-01, -1.4866e-01,\n",
      "        -2.7522e-01,  1.6156e-01, -2.0175e-01,  1.4392e-01, -8.9506e-02,\n",
      "        -1.2474e-01,  1.3857e-02, -8.5681e-02,  4.3055e-02,  1.3732e-01,\n",
      "         9.9381e-03, -1.1707e-01, -4.1152e-02, -7.0993e-02,  8.9980e-02,\n",
      "        -1.4620e-01,  5.9456e-02,  1.2137e-01,  3.7929e-02,  1.0319e-01,\n",
      "        -3.1344e-01,  1.0509e-01,  2.0067e-01, -2.9821e-01,  1.0347e-01,\n",
      "         2.5864e-01,  2.1072e-02,  7.9277e-02,  1.4978e-01,  2.3693e-02,\n",
      "         9.7559e-02])\n",
      "torch.Size([76])\n"
     ]
    }
   ],
   "source": [
    "print(expression)\n",
    "print(expression.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aab4fc4d-8c5b-465b-8933-49e0d1ace349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwf[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33b57e69-3ef7-4781-a34b-13bff0e9f2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_latent_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87e37a88-aeea-4533-8686-40f589b1a23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0173,  0.0127,  0.0038,  ...,  0.0172,  0.0074, -0.0171],\n",
      "        [ 0.0362, -0.0402, -0.0246,  ...,  0.0041,  0.0423,  0.0309],\n",
      "        [-0.0221, -0.0022, -0.0253,  ..., -0.0002, -0.0018,  0.0240],\n",
      "        ...,\n",
      "        [ 0.0081, -0.0251, -0.0101,  ...,  0.0024,  0.0366,  0.0027],\n",
      "        [ 0.0070, -0.0065, -0.0115,  ..., -0.0205,  0.0206,  0.0041],\n",
      "        [-0.0239, -0.0189, -0.0146,  ..., -0.0106, -0.0117,  0.0127]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5507, 32])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(latent_codes)\n",
    "latent_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3608c52-7459-4685-983a-7d8720d79fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,  615],\n",
       "       [   1, 3788],\n",
       "       [   2, 4448],\n",
       "       ...,\n",
       "       [5510, 3785],\n",
       "       [5511, 3432],\n",
       "       [5512, 2237]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34d86c4-3323-4798-8bbf-8a611e7da3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512,\n",
       " 512,\n",
       " array([-2.22321152e+03,  2.42276352e+03,  5.02588000e-01,  4.88307000e-01])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c9c58e1-fbf8-461f-8170-43635639113f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwf[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80affcc1-2368-41a0-b8ef-1618af2ad98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a60fe6-d61b-4fa0-ae5e-4db697f75b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-2022.05-215]",
   "language": "python",
   "name": "conda-env-.conda-2022.05-215-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
